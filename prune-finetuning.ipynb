{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogHandlers exist!\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import logging\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "import torchvision.transforms.functional as TVF\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from collections import OrderedDict\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from MSRResNet.utils import utils_logger\n",
    "from MSRResNet.utils import utils_image as util\n",
    "from MSRResNet.SRResNet import MSRResNet, PrunedMSRResNet\n",
    "from CARN import CARN\n",
    "from rdn import RDN\n",
    "from RRDBNet_arch import RRDBNet\n",
    "from RCAN import RCAN\n",
    "\n",
    "\n",
    "def get_logger(name):\n",
    "    # Set logger\n",
    "    utils_logger.logger_info(name, log_path=f'{name}.log')\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    return logger\n",
    "\n",
    "def count_num_of_parameters(model):\n",
    "    number_parameters = sum(map(lambda x: x.numel(), model.parameters()))\n",
    "    return number_parameters\n",
    "\n",
    "def _load_img_array(path, color_mode='RGB',\n",
    "                    channel_mean=None, modcrop=[0, 0, 0, 0]):\n",
    "    '''Load an image using PIL and convert it into specified color space,\n",
    "    and return it as an numpy array.\n",
    "    https://github.com/fchollet/keras/blob/master/keras/preprocessing/image.py\n",
    "    The code is modified from Keras.preprocessing.image.load_img, img_to_array.\n",
    "    '''\n",
    "    # Load image\n",
    "    img = Image.open(path)\n",
    "    if color_mode == 'RGB':\n",
    "        cimg = img.convert('RGB')\n",
    "        x = np.asarray(cimg, dtype='float32')\n",
    "    elif color_mode == 'YCbCr' or color_mode == 'Y':\n",
    "        cimg = img.convert('YCbCr')\n",
    "        x = np.asarray(cimg, dtype='float32')\n",
    "        if color_mode == 'Y':\n",
    "            x = x[:, :, 0:1]\n",
    "    # Normalize To 0-1\n",
    "    x *= 1.0 / 255.0\n",
    "    if channel_mean:\n",
    "        x[:, :, 0] -= channel_mean[0]\n",
    "        x[:, :, 1] -= channel_mean[1]\n",
    "        x[:, :, 2] -= channel_mean[2]\n",
    "    if modcrop[0] * modcrop[1] * modcrop[2] * modcrop[3]:\n",
    "        x = x[modcrop[0]:-modcrop[1], modcrop[2]:-modcrop[3], :]\n",
    "    return x\n",
    "\n",
    "def _rgb2ycbcr(img, maxVal=255):\n",
    "    # Same as MATLAB's rgb2ycbcr\n",
    "    # Updated at 03/14/2017\n",
    "    # Not tested for cb and cr\n",
    "    O = np.array([[16],\n",
    "                  [128],\n",
    "                  [128]])\n",
    "    T = np.array([[0.256788235294118, 0.504129411764706, 0.097905882352941],\n",
    "                  [-0.148223529411765, -0.290992156862745, 0.439215686274510],\n",
    "                  [0.439215686274510, -0.367788235294118, -0.071427450980392]])\n",
    "    if maxVal == 1:\n",
    "        O = O / 255.0\n",
    "    t = np.reshape(img, (img.shape[0] * img.shape[1], img.shape[2]))\n",
    "    t = np.dot(t, np.transpose(T))\n",
    "    t[:, 0] += O[0]\n",
    "    t[:, 1] += O[1]\n",
    "    t[:, 2] += O[2]\n",
    "    ycbcr = np.reshape(t, [img.shape[0], img.shape[1], img.shape[2]])\n",
    "    return ycbcr\n",
    "\n",
    "def psnr(y_true, y_pred, shave_border=4):\n",
    "    '''\n",
    "        Input must be 0-255, 2D\n",
    "    '''\n",
    "    target_data = np.array(y_true, dtype=np.float32)\n",
    "    ref_data = np.array(y_pred, dtype=np.float32)\n",
    "    diff = ref_data - target_data\n",
    "    if shave_border > 0:\n",
    "        diff = diff[shave_border:-shave_border, shave_border:-shave_border]\n",
    "    rmse = np.sqrt(np.mean(np.power(diff, 2)))\n",
    "    return 20 * np.log10(255. / rmse)\n",
    "\n",
    "def calculate_psnr(testsets_H, testset_O, logger):\n",
    "    count = 0\n",
    "    avg_psnr_value = 0.0\n",
    "    h_list = sorted(list(Path(testsets_H).glob('*.png')))\n",
    "    o_list = sorted(list(Path(testset_O).glob('*.png')))\n",
    "    for h_path, o_path in tqdm(zip(h_list, o_list)):\n",
    "        with torch.no_grad():\n",
    "            # Load label image\n",
    "            h_img = _load_img_array(h_path)\n",
    "            h_img = (h_img * 255).astype(np.uint8)\n",
    "            # Load output image\n",
    "            o_img = _load_img_array(o_path)\n",
    "            o_img = (o_img * 255).astype(np.uint8)\n",
    "            # Get psnr of bicubic, predicted\n",
    "            psnr_value = psnr(h_img,#_rgb2ycbcr(h_img)[:, :, 0],\n",
    "                              o_img,#_rgb2ycbcr(o_img)[:, :, 0],\n",
    "                              4)\n",
    "            count += 1\n",
    "            avg_psnr_value += psnr_value\n",
    "    return (avg_psnr_value / count)\n",
    "\n",
    "# Initialize variables\n",
    "data_dir='../dataset'\n",
    "root_dir='MSRResNet'\n",
    "save=False\n",
    "# --------------------------------\n",
    "# basic settings\n",
    "# --------------------------------\n",
    "testsets = f'{data_dir}/DIV2K'\n",
    "testset_L = f'DIV2K_valid_LR_bicubic'\n",
    "testset_H = f'DIV2K_valid_HR'\n",
    "L_folder = os.path.join(testsets, testset_L, 'X4')\n",
    "H_folder = os.path.join(testsets, testset_H)\n",
    "E_folder = os.path.join('results')\n",
    "P_folder = os.path.join('pruned')\n",
    "torch.cuda.current_device()\n",
    "torch.cuda.empty_cache()\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger = get_logger('AIM-track')\n",
    "\n",
    "model_name = 'RRDBNet_4'\n",
    "model_path = os.path.join('model/RRDB_v16_e17000.pth')\n",
    "#rewind_path = os.path.join('RCAN_v2_e0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params number: 66227\n"
     ]
    }
   ],
   "source": [
    "def load_model(model_path, device, name='MSRResNet'):\n",
    "    # --------------------------------\n",
    "    # load model\n",
    "    # --------------------------------\n",
    "    if name == 'MSRResNet':\n",
    "        model = MSRResNet(in_nc=3, out_nc=3, nf=64, nb=16, upscale=4)\n",
    "        model.load_state_dict(torch.load(model_path), strict=True)\n",
    "        model.eval()\n",
    "        for k, v in model.named_parameters():\n",
    "            v.requires_grad = False\n",
    "        model = model.to(device)\n",
    "        return model\n",
    "    elif name == 'PMSRResNet':\n",
    "        model = torch.nn.DataParallel(PrunedMSRResNet())\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint['net'], strict=True)\n",
    "        model.eval()\n",
    "        for k, v in model.named_parameters():\n",
    "            v.requires_grad = False\n",
    "        model = model.to(device)\n",
    "        return model\n",
    "    elif name == 'CARN':\n",
    "        model = torch.nn.DataParallel(CARN(multi_scale=4, group=1))\n",
    "        # Load pretrained model\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint['net'])\n",
    "        # Return net\n",
    "        model.eval()\n",
    "        for k, v in model.named_parameters():\n",
    "            v.requires_grad = False\n",
    "        model = model.to(device)\n",
    "        return model\n",
    "    elif name == 'PCARN':\n",
    "        model = CARN(multi_scale=4, group=1, channel_cnt=12)\n",
    "        # Load pretrained model\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint['net'])\n",
    "        # Return net\n",
    "        model.eval()\n",
    "        for k, v in model.named_parameters():\n",
    "            v.requires_grad = False\n",
    "        model = model.to(device)\n",
    "        return model\n",
    "    elif name == 'PCARN32':\n",
    "        #model = torch.nn.DataParallel(CARN(multi_scale=4, group=1, channel_cnt=32))\n",
    "        model = CARN(multi_scale=4, group=1, channel_cnt=32)\n",
    "        # Load pretrained model\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint['net'])\n",
    "        # Return net\n",
    "        model.eval()\n",
    "        for k, v in model.named_parameters():\n",
    "            v.requires_grad = False\n",
    "        model = model.to(device)\n",
    "        #modle = model.module\n",
    "        return model\n",
    "    elif name == 'PCARN6':\n",
    "        model = CARN(multi_scale=4, group=1, channel_cnt=6)\n",
    "        # Load pretrained model\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint['net'])\n",
    "        # Return net\n",
    "        model.eval()\n",
    "        for k, v in model.named_parameters():\n",
    "            v.requires_grad = False\n",
    "        model = model.to(device)\n",
    "        return model\n",
    "    elif name == 'SRDenseNet':\n",
    "        model = torch.load(model_path)['model']\n",
    "        # Return net\n",
    "        model.eval()\n",
    "        for k, v in model.named_parameters():\n",
    "            v.requires_grad = False\n",
    "        model = model.to(device)\n",
    "        return model\n",
    "    elif name == 'RDN':\n",
    "        model = RDN(scale_factor=4, num_channels=3, num_features=64, growth_rate=64, num_blocks=16, num_layers=8)\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint['net'])\n",
    "        model.eval()\n",
    "        for k, v in model.named_parameters():\n",
    "            v.requires_grad = False\n",
    "        model = model.to(device)\n",
    "        return model\n",
    "    elif name == 'PRDN_58':\n",
    "        model = torch.nn.DataParallel(RDN(scale_factor=4, num_channels=3, num_features=58, growth_rate=58, num_blocks=16, num_layers=8))\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint['net'])\n",
    "        model.eval()\n",
    "        for k, v in model.named_parameters():\n",
    "            v.requires_grad = False\n",
    "        model = model.to(device)\n",
    "        model = model.module\n",
    "        return model\n",
    "    elif name == 'PRDN_52':\n",
    "        model = torch.nn.DataParallel(RDN(scale_factor=4, num_channels=3, num_features=52, growth_rate=52, num_blocks=16, num_layers=8))\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint['net'])\n",
    "        model.eval()\n",
    "        for k, v in model.named_parameters():\n",
    "            v.requires_grad = False\n",
    "        model = model.to(device)\n",
    "        model = model.module\n",
    "        return model\n",
    "    elif name == 'PRDN_46':\n",
    "        model = torch.nn.DataParallel(RDN(scale_factor=4, num_channels=3, num_features=46, growth_rate=46, num_blocks=16, num_layers=8))\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint['net'])\n",
    "        model.eval()\n",
    "        for k, v in model.named_parameters():\n",
    "            v.requires_grad = False\n",
    "        model = model.to(device)\n",
    "        model = model.module\n",
    "        return model\n",
    "    elif name == 'PRDN_40':\n",
    "        model = torch.nn.DataParallel(RDN(scale_factor=4, num_channels=3, num_features=40, growth_rate=40, num_blocks=16, num_layers=8))\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint['net'])\n",
    "        model.eval()\n",
    "        for k, v in model.named_parameters():\n",
    "            v.requires_grad = False\n",
    "        model = model.to(device)\n",
    "        model = model.module\n",
    "        return model\n",
    "    elif name == 'PRDN_34':\n",
    "        model = RDN(scale_factor=4, num_channels=3, num_features=34, growth_rate=34, num_blocks=16, num_layers=8)\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint['net'])\n",
    "        model.eval()\n",
    "        for k, v in model.named_parameters():\n",
    "            v.requires_grad = False\n",
    "        model = model.to(device)\n",
    "        return model\n",
    "    elif name == 'PRDN_32':\n",
    "        model = torch.nn.DataParallel(RDN(scale_factor=4, num_channels=3, num_features=32, growth_rate=32, num_blocks=16, num_layers=8))\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint['net'])\n",
    "        model.eval()\n",
    "        for k, v in model.named_parameters():\n",
    "            v.requires_grad = False\n",
    "        model = model.to(device)\n",
    "        model = model.module\n",
    "        return model\n",
    "    elif name == 'PRDN_28':\n",
    "        model = RDN(scale_factor=4, num_channels=3, num_features=28, growth_rate=28, num_blocks=16, num_layers=8)\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint['net'])\n",
    "        model.eval()\n",
    "        for k, v in model.named_parameters():\n",
    "            v.requires_grad = False\n",
    "        model = model.to(device)\n",
    "        return model\n",
    "    elif name == 'PRDN_22':\n",
    "        model = RDN(scale_factor=4, num_channels=3, num_features=22, growth_rate=22, num_blocks=16, num_layers=8)\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint['net'])\n",
    "        model.eval()\n",
    "        for k, v in model.named_parameters():\n",
    "            v.requires_grad = False\n",
    "        model = model.to(device)\n",
    "        return model\n",
    "    elif name == 'PRDN_12':\n",
    "        model = torch.nn.DataParallel(RDN(scale_factor=4, num_channels=3, num_features=12, growth_rate=12, num_blocks=16, num_layers=8))\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint['net'])\n",
    "        model.eval()\n",
    "        for k, v in model.named_parameters():\n",
    "            v.requires_grad = False\n",
    "        model = model.to(device)\n",
    "        model = model.module\n",
    "        return model\n",
    "    elif name == 'PRDN_6':\n",
    "        model = torch.nn.DataParallel(RDN(scale_factor=4, num_channels=3, num_features=6, growth_rate=6, num_blocks=16, num_layers=8))\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint['net'])\n",
    "        model.eval()\n",
    "        for k, v in model.named_parameters():\n",
    "            v.requires_grad = False\n",
    "        model = model.to(device)\n",
    "        model = model.module\n",
    "        return model\n",
    "    elif name == 'RRDBNet':\n",
    "        model = RRDBNet(3, 3, 64, 23, gc=32)\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint['net'])\n",
    "        model.eval()\n",
    "        for k, v in model.named_parameters():\n",
    "            v.requires_grad = False\n",
    "        model = model.to(device)\n",
    "        return model\n",
    "    elif name == 'RRDBNet_32':\n",
    "        model = RRDBNet(3, 3, 32, 23, gc=16)\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint['net'])\n",
    "        model.eval()\n",
    "        for k, v in model.named_parameters():\n",
    "            v.requires_grad = False\n",
    "        model = model.to(device)\n",
    "        return model\n",
    "    elif name == 'RRDBNet_16':\n",
    "        model = RRDBNet(3, 3, 16, 23, gc=8)\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint['net'])\n",
    "        model.eval()\n",
    "        for k, v in model.named_parameters():\n",
    "            v.requires_grad = False\n",
    "        model = model.to(device)\n",
    "        return model\n",
    "    elif name == 'RRDBNet_12':\n",
    "        model = RRDBNet(3, 3, 12, 23, gc=6)\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint['net'])\n",
    "        model.eval()\n",
    "        for k, v in model.named_parameters():\n",
    "            v.requires_grad = False\n",
    "        model = model.to(device)\n",
    "        return model\n",
    "    elif name == 'RRDBNet_8':\n",
    "        model = RRDBNet(3, 3, 8, 23, gc=4)\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint['net'])\n",
    "        model.eval()\n",
    "        for k, v in model.named_parameters():\n",
    "            v.requires_grad = False\n",
    "        model = model.to(device)\n",
    "        return model\n",
    "    elif name == 'RRDBNet_4':\n",
    "        model = RRDBNet(3, 3, 4, 23, gc=2)\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint['net'])\n",
    "        model.eval()\n",
    "        for k, v in model.named_parameters():\n",
    "            v.requires_grad = False\n",
    "        model = model.to(device)\n",
    "        return model\n",
    "    elif name == 'RCAN':\n",
    "        model = RCAN()\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint['net'])\n",
    "        model.eval()\n",
    "        for k, v in model.named_parameters():\n",
    "            v.requires_grad = False\n",
    "        model = model.to(device)\n",
    "        return model\n",
    "\n",
    "model = load_model(model_path, device, model_name)\n",
    "print(f'Params number: {count_num_of_parameters(model)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save    1 to results/801.png\n",
      "Save    2 to results/802.png\n",
      "Save    3 to results/803.png\n",
      "Save    4 to results/804.png\n",
      "Save    5 to results/805.png\n",
      "Save    6 to results/806.png\n",
      "Save    7 to results/807.png\n",
      "Save    8 to results/808.png\n",
      "Save    9 to results/809.png\n",
      "Save   10 to results/810.png\n",
      "Save   11 to results/811.png\n",
      "Save   12 to results/812.png\n",
      "Save   13 to results/813.png\n",
      "Save   14 to results/814.png\n",
      "Save   15 to results/815.png\n",
      "Save   16 to results/816.png\n",
      "Save   17 to results/817.png\n",
      "Save   18 to results/818.png\n",
      "Save   19 to results/819.png\n",
      "Save   20 to results/820.png\n",
      "Save   21 to results/821.png\n",
      "Save   22 to results/822.png\n",
      "Save   23 to results/823.png\n",
      "Save   24 to results/824.png\n",
      "Save   25 to results/825.png\n",
      "Save   26 to results/826.png\n",
      "Save   27 to results/827.png\n",
      "Save   28 to results/828.png\n",
      "Save   29 to results/829.png\n",
      "Save   30 to results/830.png\n",
      "Save   31 to results/831.png\n",
      "Save   32 to results/832.png\n",
      "Save   33 to results/833.png\n",
      "Save   34 to results/834.png\n",
      "Save   35 to results/835.png\n",
      "Save   36 to results/836.png\n",
      "Save   37 to results/837.png\n",
      "Save   38 to results/838.png\n",
      "Save   39 to results/839.png\n",
      "Save   40 to results/840.png\n",
      "Save   41 to results/841.png\n",
      "Save   42 to results/842.png\n",
      "Save   43 to results/843.png\n",
      "Save   44 to results/844.png\n",
      "Save   45 to results/845.png\n",
      "Save   46 to results/846.png\n",
      "Save   47 to results/847.png\n",
      "Save   48 to results/848.png\n",
      "Save   49 to results/849.png\n",
      "Save   50 to results/850.png\n",
      "Save   51 to results/851.png\n",
      "Save   52 to results/852.png\n",
      "Save   53 to results/853.png\n",
      "Save   54 to results/854.png\n",
      "Save   55 to results/855.png\n",
      "Save   56 to results/856.png\n",
      "Save   57 to results/857.png\n",
      "Save   58 to results/858.png\n",
      "Save   59 to results/859.png\n",
      "Save   60 to results/860.png\n",
      "Save   61 to results/861.png\n",
      "Save   62 to results/862.png\n",
      "Save   63 to results/863.png\n",
      "Save   64 to results/864.png\n",
      "Save   65 to results/865.png\n",
      "Save   66 to results/866.png\n",
      "Save   67 to results/867.png\n",
      "Save   68 to results/868.png\n",
      "Save   69 to results/869.png\n",
      "Save   70 to results/870.png\n",
      "Save   71 to results/871.png\n",
      "Save   72 to results/872.png\n",
      "Save   73 to results/873.png\n",
      "Save   74 to results/874.png\n",
      "Save   75 to results/875.png\n",
      "Save   76 to results/876.png\n",
      "Save   77 to results/877.png\n",
      "Save   78 to results/878.png\n",
      "Save   79 to results/879.png\n",
      "Save   80 to results/880.png\n",
      "Save   81 to results/881.png\n",
      "Save   82 to results/882.png\n",
      "Save   83 to results/883.png\n",
      "Save   84 to results/884.png\n",
      "Save   85 to results/885.png\n",
      "Save   86 to results/886.png\n",
      "Save   87 to results/887.png\n",
      "Save   88 to results/888.png\n",
      "Save   89 to results/889.png\n",
      "Save   90 to results/890.png\n",
      "Save   91 to results/891.png\n",
      "Save   92 to results/892.png\n",
      "Save   93 to results/893.png\n",
      "Save   94 to results/894.png\n",
      "Save   95 to results/895.png\n",
      "Save   96 to results/896.png\n",
      "Save   97 to results/897.png\n",
      "Save   98 to results/898.png\n",
      "Save   99 to results/899.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save  100 to results/900.png\n",
      "------> Average runtime of (../dataset/DIV2K/DIV2K_valid_LR_bicubic/X4) is : 0.928608 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:49,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR before pruning 28.18131381073257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def test(model, L_folder, out_folder, logger, save, ensemble=False):\n",
    "    # --------------------------------\n",
    "    # read image\n",
    "    # --------------------------------\n",
    "    util.mkdir(out_folder)\n",
    "\n",
    "    # record PSNR, runtime\n",
    "    test_results = OrderedDict()\n",
    "    test_results['runtime'] = []\n",
    "\n",
    "    logger.info(L_folder)\n",
    "    logger.info(out_folder)\n",
    "    idx = 0\n",
    "\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    for img in util.get_image_paths(L_folder):\n",
    "\n",
    "        # --------------------------------\n",
    "        # (1) img_L\n",
    "        # --------------------------------\n",
    "        idx += 1\n",
    "        img_name, ext = os.path.splitext(os.path.basename(img))\n",
    "        logger.info('{:->4d}--> {:>10s}'.format(idx, img_name+ext))\n",
    "\n",
    "        img_L = util.imread_uint(img, n_channels=3)\n",
    "        img_L = util.uint2tensor4(img_L)\n",
    "        img_L = img_L.to(device)\n",
    "\n",
    "        if ensemble:\n",
    "            start.record()\n",
    "            # Original\n",
    "            img_E = model(img_L)\n",
    "            # Rotation 90\n",
    "            img_E += model(img_L.rot90(1, (-1, -2))).rot90(3, (-1, -2))\n",
    "            # Rotation 180\n",
    "            img_E += model(img_L.rot90(2, (-1, -2))).rot90(2, (-1, -2))\n",
    "            # Rotation 270\n",
    "            img_E += model(img_L.rot90(3, (-1, -2))).rot90(1, (-1, -2))\n",
    "            # H flip\n",
    "            img_L_hflip = img_L.flip(-1)\n",
    "            img_E += model(img_L_hflip).flip(-1)\n",
    "            # H flip + rot90\n",
    "            img_E += model(img_L_hflip.rot90(1, (-1, -2))).rot90(3, (-1, -2)).flip(-1)\n",
    "            # H flip + rot180\n",
    "            img_E += model(img_L_hflip.rot90(2, (-1, -2))).rot90(2, (-1, -2)).flip(-1)\n",
    "            # H flip + rot270\n",
    "            img_E += model(img_L_hflip.rot90(3, (-1, -2))).rot90(1, (-1, -2)).flip(-1)\n",
    "            img_E /= 8\n",
    "            end.record()\n",
    "        else:\n",
    "            start.record()\n",
    "            img_E = model(img_L)\n",
    "            end.record()\n",
    "        torch.cuda.synchronize()\n",
    "        test_results['runtime'].append(start.elapsed_time(end))  # milliseconds\n",
    "\n",
    "        # --------------------------------\n",
    "        # (2) img_E\n",
    "        # --------------------------------\n",
    "        img_E = util.tensor2uint(img_E)\n",
    "\n",
    "        if save:\n",
    "            new_name = '{:3d}'.format(int(img_name.split('x')[0]))\n",
    "            path = os.path.join(out_folder, new_name+ext)\n",
    "            print('Save {:4d} to {:10s}'.format(idx, path))\n",
    "            util.imsave(img_E, path)\n",
    "    ave_runtime = sum(test_results['runtime']) / len(test_results['runtime']) / 1000.0\n",
    "    print('------> Average runtime of ({}) is : {:.6f} seconds'.format(L_folder, ave_runtime))\n",
    "\n",
    "test(model, L_folder, E_folder, logger, True, True)\n",
    "print(f'PSNR before pruning {calculate_psnr(H_folder, E_folder, logger)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.manifold import TSNE\n",
    "from tqdm import tqdm\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (14,4)\n",
    "\n",
    "def visualize_both(name, norm):\n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "    ax[0].set_xscale('linear')\n",
    "    visualize_norm_stat(f\"{name}_lin\", norm, ax[0])\n",
    "    ax[1].set_xscale('log')\n",
    "    visualize_norm_stat(f\"{name}_log\", norm.abs(), ax[1])\n",
    "    plt.show()\n",
    "    #plt.savefig(f\"images/{name}.png\", dpi=300)\n",
    "\n",
    "def visualize_norm_stat(name, norm, ax):\n",
    "    # Draw each point\n",
    "    y = np.zeros(np.shape(norm))\n",
    "    ax.plot(norm, y, '|')\n",
    "    estimator = stats.gaussian_kde(norm, bw_method='silverman')\n",
    "    # Draw kernel density estimate\n",
    "    X = np.arange(norm.min() * 1.1, norm.max() * 1.1, 0.1)\n",
    "    K = estimator(X)\n",
    "    ax.plot(X, K, label=f'{name}')\n",
    "    # Set other things\n",
    "    ax.legend(loc='best')\n",
    "\n",
    "filter_unprune = ['module.sub_mean.shifter', 'module.add_mean.shifter']\n",
    "for name, module in model.named_modules():\n",
    "    module_name = type(module).__name__\n",
    "    unprune = 'Conv2d' not in module_name or name in filter_unprune\n",
    "    if unprune: continue\n",
    "    visualize_both(name, module.weight.sum(-1).sum(-1).sum(-1).detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def prune_with_synflow(model, origin_channel, prune_channel_list, filter_unprune, filter_in, filter_out):\n",
    "    # Copy model\n",
    "    model_new = copy.deepcopy(model)\n",
    "    model_new.train()\n",
    "    for prune_channel in prune_channel_list:\n",
    "        # Variable\n",
    "        amount = prune_channel / origin_channel\n",
    "        logger.debug(\"=\" * 20)\n",
    "        logger.debug(\"=\" * 20)\n",
    "        logger.debug(f\"{origin_channel} to {origin_channel - prune_channel} with amount: {amount}\")\n",
    "        logger.debug(\"=\" * 20)\n",
    "        logger.debug(\"=\" * 20)\n",
    "        origin_channel -= prune_channel\n",
    "        # Get absolute model\n",
    "        model_abs = copy.deepcopy(model_new)\n",
    "        for name, module in model_abs.named_modules():\n",
    "            # absolute or not\n",
    "            module_name = type(module).__name__\n",
    "            unabsolute = 'Conv2d' not in module_name\n",
    "            if unabsolute: continue\n",
    "            # absolute\n",
    "            module.weight = torch.nn.Parameter(module.weight.abs() * 1e-2)\n",
    "            module.bias = torch.nn.Parameter(torch.zeros_like(module.bias))\n",
    "        # SynFlow\n",
    "        model_abs.zero_grad()\n",
    "        img = torch.ones((1, 3, 32, 32))\n",
    "        out = model_abs(img)\n",
    "        loss = out.sum()\n",
    "        loss.backward()\n",
    "        # do pruning\n",
    "        for name, module in model_new.named_modules():\n",
    "            # Prune or not\n",
    "            module_name = type(module).__name__\n",
    "            unprune = 'Conv2d' not in module_name or name in filter_unprune\n",
    "            logger.debug(\"=\" * 20)\n",
    "            logger.debug(f\"{name}, {module_name}: Prune-{not unprune}\")\n",
    "            if unprune: continue\n",
    "            # Get absolute module with gradient\n",
    "            module_abs = None\n",
    "            for n, m in model_abs.named_modules():\n",
    "                if name == n:\n",
    "                    module_abs = m\n",
    "                    break\n",
    "            # Prepare pruning\n",
    "            pre_module_weight_shape = module.weight.shape\n",
    "            pre_module_bias_shape = module.bias.shape\n",
    "            # SynFlow score\n",
    "            module_abs.weight = torch.nn.Parameter(torch.mul(module_abs.weight, module_abs.weight.grad))\n",
    "            # Prune in_channel\n",
    "            if name not in filter_in:\n",
    "                prune.ln_structured(module_abs, 'weight', amount=amount, n=2, dim=1)\n",
    "                prune.remove(module_abs, 'weight')\n",
    "                in_channel_mask = module_abs.weight.sum(-1).sum(-1).sum(0) != 0\n",
    "            else:\n",
    "                in_channel_mask = torch.ones(module.weight.shape[1], dtype=torch.bool).to(device)\n",
    "            # Prune out_channel\n",
    "            if name not in filter_out:\n",
    "                prune.ln_structured(module_abs, 'weight', amount=amount, n=2, dim=0)\n",
    "                prune.remove(module_abs, 'weight')\n",
    "                out_channel_mask = module_abs.weight.sum(-1).sum(-1).sum(-1) != 0\n",
    "            else:\n",
    "                out_channel_mask = torch.ones(module.weight.shape[0], dtype=torch.bool).to(device)\n",
    "            module.weight = torch.nn.Parameter(module.weight[:,in_channel_mask,:,:])\n",
    "            module.weight = torch.nn.Parameter(module.weight[out_channel_mask,:,:,:])\n",
    "            module.bias = torch.nn.Parameter(module.bias[out_channel_mask])\n",
    "            logger.debug(f\"mask_index.shape: {out_channel_mask.shape}\")\n",
    "            logger.debug(f\"module.weight.shape: {pre_module_weight_shape} --> {module.weight.shape}\")\n",
    "            logger.debug(f\"module.bias.shape: {pre_module_bias_shape} --> {module.bias.shape}\")\n",
    "    return model_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "\n",
    "def prune_with_srflow(model, origin_channel, prune_channel_list, filter_unprune, filter_in, filter_out):\n",
    "    # Copy model\n",
    "    model_new = copy.deepcopy(model)\n",
    "    model_new.train()\n",
    "    for prune_channel in prune_channel_list:\n",
    "        # Variable\n",
    "        amount = prune_channel / origin_channel\n",
    "        logger.debug(\"=\" * 20)\n",
    "        logger.debug(\"=\" * 20)\n",
    "        logger.debug(f\"{origin_channel} to {origin_channel - prune_channel} with amount: {amount}\")\n",
    "        logger.debug(\"=\" * 20)\n",
    "        logger.debug(\"=\" * 20)\n",
    "        origin_channel -= prune_channel\n",
    "        # SRFlow\n",
    "        model_flow = copy.deepcopy(model_new)\n",
    "        model_flow.zero_grad()\n",
    "        img_lr = torch.ones((1, 3, 32, 32))\n",
    "        img_hr = torch.ones((1, 3, 32 * 4, 32 * 4))\n",
    "        img_sr = model_flow(img_lr)\n",
    "        loss = torch.nn.MSELoss()(img_hr, img_sr)\n",
    "        loss.backward()\n",
    "        # do pruning\n",
    "        for name, module in model_new.named_modules():\n",
    "            # Prune or not\n",
    "            module_name = type(module).__name__\n",
    "            unprune = 'Conv2d' not in module_name or name in filter_unprune\n",
    "            logger.debug(\"=\" * 20)\n",
    "            logger.debug(f\"{name}, {module_name}: Prune-{not unprune}\")\n",
    "            if unprune: continue\n",
    "            # Get flow module with gradient\n",
    "            module_flow = None\n",
    "            for n, m in model_flow.named_modules():\n",
    "                if name == n:\n",
    "                    module_flow = m\n",
    "                    break\n",
    "            # Prepare pruning\n",
    "            pre_module_weight_shape = module.weight.shape\n",
    "            pre_module_bias_shape = module.bias.shape\n",
    "            # SRFlow score\n",
    "            module_flow.weight = torch.nn.Parameter(torch.mul(module_flow.weight.abs(), module_flow.weight.grad.abs()))\n",
    "            # Prune in_channel\n",
    "            if name not in filter_in:\n",
    "                prune.ln_structured(module_flow, 'weight', amount=amount, n=2, dim=1)\n",
    "                prune.remove(module_flow, 'weight')\n",
    "                in_channel_mask = module_flow.weight.sum(-1).sum(-1).sum(0) != 0\n",
    "            else:\n",
    "                in_channel_mask = torch.ones(module.weight.shape[1], dtype=torch.bool).to(device)\n",
    "            # Prune out_channel\n",
    "            if name not in filter_out:\n",
    "                prune.ln_structured(module_flow, 'weight', amount=amount, n=2, dim=0)\n",
    "                prune.remove(module_flow, 'weight')\n",
    "                out_channel_mask = module_flow.weight.sum(-1).sum(-1).sum(-1) != 0\n",
    "            else:\n",
    "                out_channel_mask = torch.ones(module.weight.shape[0], dtype=torch.bool).to(device)\n",
    "            module.weight = torch.nn.Parameter(module.weight[:,in_channel_mask,:,:])\n",
    "            module.weight = torch.nn.Parameter(module.weight[out_channel_mask,:,:,:])\n",
    "            module.bias = torch.nn.Parameter(module.bias[out_channel_mask])\n",
    "            logger.debug(f\"mask_index.shape: {out_channel_mask.shape}\")\n",
    "            logger.debug(f\"module.weight.shape: {pre_module_weight_shape} --> {module.weight.shape}\")\n",
    "            logger.debug(f\"module.bias.shape: {pre_module_bias_shape} --> {module.bias.shape}\")\n",
    "    return model_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "\n",
    "\n",
    "def prune_with_srflow2(model, origin_channel, prune_channel_list, filter_unprune, filter_in, filter_out, device, epoch=50):\n",
    "    # Copy model\n",
    "    model_new = copy.deepcopy(model)\n",
    "    model_new.train()\n",
    "    # Image\n",
    "    img_lr = torch.ones((1, 3, 32, 32)).to(device)\n",
    "    img_hr = torch.ones((1, 3, 32 * 4, 32 * 4)).to(device)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    for i, prune_channel in enumerate(prune_channel_list):\n",
    "        # Variable\n",
    "        amount = prune_channel / origin_channel\n",
    "        logger.debug(\"=\" * 20)\n",
    "        logger.debug(\"=\" * 20)\n",
    "        logger.debug(f\"{origin_channel} to {origin_channel - prune_channel} with amount: {amount}\")\n",
    "        logger.debug(\"=\" * 20)\n",
    "        logger.debug(\"=\" * 20)\n",
    "        origin_channel -= prune_channel\n",
    "        # Train model a little bit\n",
    "        optimizer = torch.optim.Adam(model_new.parameters())\n",
    "        for j in range(epoch):\n",
    "            optimizer.zero_grad()\n",
    "            img_sr = model_new(img_lr)\n",
    "            loss = criterion(img_hr, img_sr)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f\"[{i + 1}/{len(prune_channel_list)}][{j + 1}/{epoch}] loss: {loss}\")\n",
    "        # SRFlow\n",
    "        model_flow = copy.deepcopy(model_new)\n",
    "        model_flow.zero_grad()\n",
    "        img_sr = model_flow(img_lr)\n",
    "        loss = criterion(img_hr, img_sr)\n",
    "        loss.backward()\n",
    "        # do pruning\n",
    "        for name, module in model_new.named_modules():\n",
    "            # Prune or not\n",
    "            module_name = type(module).__name__\n",
    "            unprune = 'Conv2d' not in module_name or name in filter_unprune\n",
    "            logger.debug(\"=\" * 20)\n",
    "            logger.debug(f\"{name}, {module_name}: Prune-{not unprune}\")\n",
    "            if unprune: continue\n",
    "            # Get flow module with gradient\n",
    "            module_flow = None\n",
    "            for n, m in model_flow.named_modules():\n",
    "                if name == n:\n",
    "                    module_flow = m\n",
    "                    break\n",
    "            # Prepare pruning\n",
    "            pre_module_weight_shape = module.weight.shape\n",
    "            pre_module_bias_shape = module.bias.shape\n",
    "            # SRFlow score\n",
    "            module_flow.weight = torch.nn.Parameter(torch.mul(module_flow.weight.abs(), module_flow.weight.grad.abs()))\n",
    "            # Prune in_channel\n",
    "            if name not in filter_in:\n",
    "                prune.ln_structured(module_flow, 'weight', amount=amount, n=2, dim=1)\n",
    "                prune.remove(module_flow, 'weight')\n",
    "                in_channel_mask = module_flow.weight.sum(-1).sum(-1).sum(0) != 0\n",
    "            else:\n",
    "                in_channel_mask = torch.ones(module.weight.shape[1], dtype=torch.bool).to(device)\n",
    "            # Prune out_channel\n",
    "            if name not in filter_out:\n",
    "                prune.ln_structured(module_flow, 'weight', amount=amount, n=2, dim=0)\n",
    "                prune.remove(module_flow, 'weight')\n",
    "                out_channel_mask = module_flow.weight.sum(-1).sum(-1).sum(-1) != 0\n",
    "            else:\n",
    "                out_channel_mask = torch.ones(module.weight.shape[0], dtype=torch.bool).to(device)\n",
    "            module.weight = torch.nn.Parameter(module.weight[:,in_channel_mask,:,:])\n",
    "            module.weight = torch.nn.Parameter(module.weight[out_channel_mask,:,:,:])\n",
    "            module.bias = torch.nn.Parameter(module.bias[out_channel_mask])\n",
    "            logger.debug(f\"mask_index.shape: {out_channel_mask.shape}\")\n",
    "            logger.debug(f\"module.weight.shape: {pre_module_weight_shape} --> {module.weight.shape}\")\n",
    "            logger.debug(f\"module.bias.shape: {pre_module_bias_shape} --> {module.bias.shape}\")\n",
    "    # Last fine-tuning\n",
    "    optimizer = torch.optim.Adam(model_new.parameters())\n",
    "    for j in range(epoch):\n",
    "        optimizer.zero_grad()\n",
    "        img_sr = model_new(img_lr)\n",
    "        loss = criterion(img_hr, img_sr)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"[{j + 1}/{epoch}] loss: {loss}\")\n",
    "    return model_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "\n",
    "def prune_with_random(model, origin_channel, prune_channel_list, filter_unprune, filter_in, filter_out):\n",
    "    # Copy model\n",
    "    model_new = copy.deepcopy(model)\n",
    "    model_new.train()\n",
    "    for prune_channel in prune_channel_list:\n",
    "        # Variable\n",
    "        amount = prune_channel / origin_channel\n",
    "        logger.debug(\"=\" * 20)\n",
    "        logger.debug(\"=\" * 20)\n",
    "        logger.debug(f\"{origin_channel} to {origin_channel - prune_channel} with amount: {amount}\")\n",
    "        logger.debug(\"=\" * 20)\n",
    "        logger.debug(\"=\" * 20)\n",
    "        origin_channel -= prune_channel\n",
    "        # do pruning\n",
    "        for name, module in model_new.named_modules():\n",
    "            # Prune or not\n",
    "            module_name = type(module).__name__\n",
    "            unprune = 'Conv2d' not in module_name or name in filter_unprune\n",
    "            logger.debug(\"=\" * 20)\n",
    "            logger.debug(f\"{name}, {module_name}: Prune-{not unprune}\")\n",
    "            if unprune: continue\n",
    "            # Prepare pruning\n",
    "            pre_module_weight_shape = module.weight.shape\n",
    "            pre_module_bias_shape = module.bias.shape\n",
    "            # Prune in_channel\n",
    "            if name not in filter_in:\n",
    "                prune.random_structured(module, 'weight', amount=amount, dim=1)\n",
    "                prune.remove(module, 'weight')\n",
    "                in_channel_mask = module.weight.sum(-1).sum(-1).sum(0) != 0\n",
    "            else:\n",
    "                in_channel_mask = torch.ones(module.weight.shape[1], dtype=torch.bool).to(device)\n",
    "            # Prune out_channel\n",
    "            if name not in filter_out:\n",
    "                prune.random_structured(module, 'weight', amount=amount, dim=0)\n",
    "                prune.remove(module, 'weight')\n",
    "                out_channel_mask = module.weight.sum(-1).sum(-1).sum(-1) != 0\n",
    "            else:\n",
    "                out_channel_mask = torch.ones(module.weight.shape[0], dtype=torch.bool).to(device)\n",
    "            module.weight = torch.nn.Parameter(module.weight[:,in_channel_mask,:,:])\n",
    "            module.weight = torch.nn.Parameter(module.weight[out_channel_mask,:,:,:])\n",
    "            module.bias = torch.nn.Parameter(module.bias[out_channel_mask])\n",
    "            logger.debug(f\"mask_index.shape: {out_channel_mask.shape}\")\n",
    "            logger.debug(f\"module.weight.shape: {pre_module_weight_shape} --> {module.weight.shape}\")\n",
    "            logger.debug(f\"module.bias.shape: {pre_module_bias_shape} --> {module.bias.shape}\")\n",
    "    return model_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_channel = 64\n",
    "prune_channel_list = [2] * (16)\n",
    "compression_rate = ((origin_channel - sum(prune_channel_list)) / origin_channel) ** 2\n",
    "filter_unprune = []\n",
    "filter_in = ['conv_first']\n",
    "filter_out = ['conv_last']\n",
    "\n",
    "model = RRDBNet(3, 3, 64, 23, gc=32).to(device)\n",
    "print(f'Params number(Before Pruning): {count_num_of_parameters(model)}')\n",
    "print(f'compression_rate: {compression_rate}')\n",
    "print(f'Params number(Predict): {count_num_of_parameters(model) * compression_rate}')\n",
    "#pruned_model = prune_with_synflow(model, origin_channel, prune_channel_list, filter_unprune, filter_in, filter_out)\n",
    "#pruned_model = prune_with_srflow(model, origin_channel, prune_channel_list, filter_unprune, filter_in, filter_out)\n",
    "pruned_model = prune_with_srflow2(model, origin_channel, prune_channel_list, filter_unprune, filter_in, filter_out, device)\n",
    "#pruned_model = prune_with_random(model, origin_channel, prune_channel_list, filter_unprune, filter_in, filter_out)\n",
    "print(f'Params number(After Pruning): {count_num_of_parameters(pruned_model)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_channel = 64\n",
    "prune_channel_list = [2] * (16 + 8)\n",
    "compression_rate = ((origin_channel - sum(prune_channel_list)) / origin_channel) ** 2\n",
    "filter_unprune = []\n",
    "filter_in = ['sfe1']\n",
    "filter_out = ['output']\n",
    "\n",
    "model = RDN(scale_factor=4, num_channels=3, num_features=64, growth_rate=64, num_blocks=16, num_layers=8)\n",
    "print(f'Params number(Before Pruning): {count_num_of_parameters(model)}')\n",
    "print(f'compression_rate: {compression_rate}')\n",
    "print(f'Params number(Predict): {count_num_of_parameters(model) * compression_rate}')\n",
    "pruned_model = prune_with_synflow(model, origin_channel, prune_channel_list, filter_unprune, filter_in, filter_out)\n",
    "print(f'Params number(After Pruning): {count_num_of_parameters(pruned_model)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RCAN naive l2 channel pruning\n",
    "\n",
    "rewind_model = load_model(rewind_path, device, model_name)\n",
    "model_new = copy.deepcopy(model)\n",
    "# Kind of filter\n",
    "filter_unprune = ['sub_mean', 'add_mean']\n",
    "filter_in = ['head.0']\n",
    "filter_out = ['tail.1']\n",
    "amount = 0.25\n",
    "# Pruning!\n",
    "mask_dict = OrderedDict()\n",
    "logger.info('Params number(Before prune): {}'.format(count_num_of_parameters(model_new)))\n",
    "for name, module in model_new.named_modules():\n",
    "    # Prune or not\n",
    "    module_name = type(module).__name__\n",
    "    unprune = 'Conv2d' not in module_name or name in filter_unprune\n",
    "    logger.debug(\"=\" * 20)\n",
    "    logger.debug(f\"{name}, {module_name}: Prune-{not unprune}\")\n",
    "    if unprune: continue\n",
    "    # Prepare pruning\n",
    "    rewind_module = None\n",
    "    for n, m in rewind_model.named_modules():\n",
    "        if name == n:\n",
    "            rewind_module = m\n",
    "            break\n",
    "    pre_module_weight_shape = module.weight.shape\n",
    "    pre_module_bias_shape = module.bias.shape\n",
    "    # Prune in_channel\n",
    "    if name not in filter_in:\n",
    "        temp_module = copy.deepcopy(module)\n",
    "        prune.ln_structured(temp_module, 'weight', amount=amount, n=2, dim=1)\n",
    "        prune.remove(temp_module, 'weight')\n",
    "        in_channel_mask = temp_module.weight.sum(-1).sum(-1).sum(0) != 0\n",
    "    else:\n",
    "        in_channel_mask = torch.ones(module.weight.shape[1], dtype=torch.bool).to(device)\n",
    "    # Prune out_channel\n",
    "    if name not in filter_out:\n",
    "        temp_module = copy.deepcopy(module)\n",
    "        prune.ln_structured(temp_module, 'weight', amount=amount, n=2, dim=0)\n",
    "        prune.remove(temp_module, 'weight')\n",
    "        out_channel_mask = temp_module.weight.sum(-1).sum(-1).sum(-1) != 0\n",
    "    else:\n",
    "        out_channel_mask = torch.ones(module.weight.shape[0], dtype=torch.bool).to(device)\n",
    "    module.weight = torch.nn.Parameter(rewind_module.weight[:, in_channel_mask])\n",
    "    module.weight = torch.nn.Parameter(module.weight[out_channel_mask, :])\n",
    "    module.bias = torch.nn.Parameter(rewind_module.bias[out_channel_mask])\n",
    "    logger.debug(f\"mask_index.shape: {out_channel_mask.shape}\")\n",
    "    logger.debug(f\"module.weight.shape: {pre_module_weight_shape} --> {module.weight.shape}\")\n",
    "    logger.debug(f\"module.bias.shape: {pre_module_bias_shape} --> {module.bias.shape}\")\n",
    "logger.info('Params number(After prune): {}'.format(count_num_of_parameters(model_new)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RRDB naive l2 channel pruning\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "\n",
    "rewind_model = load_model(rewind_path, device, model_name)\n",
    "model_new = copy.deepcopy(model)\n",
    "# Kind of filter\n",
    "filter_unprune = []\n",
    "filter_in = ['conv_first']\n",
    "filter_out = ['conv_last']\n",
    "amount = 0.25\n",
    "# Pruning!\n",
    "mask_dict = OrderedDict()\n",
    "logger.info('Params number(Before prune): {}'.format(count_num_of_parameters(model_new)))\n",
    "for name, module in model_new.named_modules():\n",
    "    # Prune or not\n",
    "    module_name = type(module).__name__\n",
    "    unprune = 'Conv2d' not in module_name or name in filter_unprune\n",
    "    logger.debug(\"=\" * 20)\n",
    "    logger.debug(f\"{name}, {module_name}: Prune-{not unprune}\")\n",
    "    if unprune: continue\n",
    "    # Prepare pruning\n",
    "    rewind_module = None\n",
    "    for n, m in rewind_model.named_modules():\n",
    "        if name == n:\n",
    "            rewind_module = m\n",
    "            break\n",
    "    pre_module_weight_shape = module.weight.shape\n",
    "    pre_module_bias_shape = module.bias.shape\n",
    "    # Prune in_channel\n",
    "    if name not in filter_in:\n",
    "        temp_module = copy.deepcopy(module)\n",
    "        prune.ln_structured(temp_module, 'weight', amount=amount, n=2, dim=1)\n",
    "        prune.remove(temp_module, 'weight')\n",
    "        in_channel_mask = temp_module.weight.sum(-1).sum(-1).sum(0) != 0\n",
    "    else:\n",
    "        in_channel_mask = torch.ones(module.weight.shape[1], dtype=torch.bool).to(device)\n",
    "    # Prune out_channel\n",
    "    if name not in filter_out:\n",
    "        temp_module = copy.deepcopy(module)\n",
    "        prune.ln_structured(temp_module, 'weight', amount=amount, n=2, dim=0)\n",
    "        prune.remove(temp_module, 'weight')\n",
    "        out_channel_mask = temp_module.weight.sum(-1).sum(-1).sum(-1) != 0\n",
    "    else:\n",
    "        out_channel_mask = torch.ones(module.weight.shape[0], dtype=torch.bool).to(device)\n",
    "    module.weight = torch.nn.Parameter(rewind_module.weight[:, in_channel_mask])\n",
    "    module.weight = torch.nn.Parameter(module.weight[out_channel_mask, :])\n",
    "    module.bias = torch.nn.Parameter(rewind_module.bias[out_channel_mask])\n",
    "    logger.debug(f\"mask_index.shape: {out_channel_mask.shape}\")\n",
    "    logger.debug(f\"module.weight.shape: {pre_module_weight_shape} --> {module.weight.shape}\")\n",
    "    logger.debug(f\"module.bias.shape: {pre_module_bias_shape} --> {module.bias.shape}\")\n",
    "logger.info('Params number(After prune): {}'.format(count_num_of_parameters(model_new)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDN naive l2 channel pruning\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "\n",
    "rewind_model = load_model(rewind_path, device, model_name)\n",
    "model_new = copy.deepcopy(model)\n",
    "# Kind of filter\n",
    "filter_unprune = []\n",
    "filter_in = ['sfe1']\n",
    "filter_out = ['output']\n",
    "amount = 0.21428571428571428571428571428571\n",
    "# Pruning!\n",
    "mask_dict = OrderedDict()\n",
    "logger.info('Params number(Before prune): {}'.format(count_num_of_parameters(model_new)))\n",
    "for name, module in model_new.named_modules():\n",
    "    # Prune or not\n",
    "    module_name = type(module).__name__\n",
    "    unprune = 'Conv2d' not in module_name or name in filter_unprune\n",
    "    logger.debug(\"=\" * 20)\n",
    "    logger.debug(f\"{name}, {module_name}: Prune-{not unprune}\")\n",
    "    if unprune: continue\n",
    "    # Prepare pruning\n",
    "    rewind_module = None\n",
    "    for n, m in rewind_model.named_modules():\n",
    "        if name == n:\n",
    "            rewind_module = m\n",
    "            break\n",
    "    pre_module_weight_shape = module.weight.shape\n",
    "    pre_module_bias_shape = module.bias.shape\n",
    "    # Prune in_channel\n",
    "    if name not in filter_in:\n",
    "        temp_module = copy.deepcopy(module)\n",
    "        prune.ln_structured(temp_module, 'weight', amount=amount, n=2, dim=1)\n",
    "        prune.remove(temp_module, 'weight')\n",
    "        in_channel_mask = temp_module.weight.sum(-1).sum(-1).sum(0) != 0\n",
    "    else:\n",
    "        in_channel_mask = torch.ones(module.weight.shape[1], dtype=torch.bool).to(device)\n",
    "    # Prune out_channel\n",
    "    if name not in filter_out:\n",
    "        temp_module = copy.deepcopy(module)\n",
    "        prune.ln_structured(temp_module, 'weight', amount=amount, n=2, dim=0)\n",
    "        prune.remove(temp_module, 'weight')\n",
    "        out_channel_mask = temp_module.weight.sum(-1).sum(-1).sum(-1) != 0\n",
    "    else:\n",
    "        out_channel_mask = torch.ones(module.weight.shape[0], dtype=torch.bool).to(device)\n",
    "    module.weight = torch.nn.Parameter(rewind_module.weight[:, in_channel_mask])\n",
    "    module.weight = torch.nn.Parameter(module.weight[out_channel_mask, :])\n",
    "    module.bias = torch.nn.Parameter(rewind_module.bias[out_channel_mask])\n",
    "    logger.debug(f\"mask_index.shape: {out_channel_mask.shape}\")\n",
    "    logger.debug(f\"module.weight.shape: {pre_module_weight_shape} --> {module.weight.shape}\")\n",
    "    logger.debug(f\"module.bias.shape: {pre_module_bias_shape} --> {module.bias.shape}\")\n",
    "logger.info('Params number(After prune): {}'.format(count_num_of_parameters(model_new)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune CARN naively\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "\n",
    "rewind_model = load_model(rewind_path, device, model_name)\n",
    "rewind_model = rewind_model.module\n",
    "model_new = copy.deepcopy(model)\n",
    "model_new = model_new.module\n",
    "# Kind of filter\n",
    "filter_unprune = ['sub_mean.shifter', 'add_mean.shifter']\n",
    "# Pruning!\n",
    "mask_dict = OrderedDict()\n",
    "logger.info('Params number(Before prune): {}'.format(count_num_of_parameters(model_new)))\n",
    "for name, module in model_new.named_modules():\n",
    "    # Prune or not\n",
    "    module_name = type(module).__name__\n",
    "    unprune = 'Conv2d' not in module_name or name in filter_unprune\n",
    "    logger.debug(\"=\" * 20)\n",
    "    logger.debug(f\"{name}, {module_name}: Prune-{not unprune}\")\n",
    "    if unprune: continue\n",
    "    # Prepare pruning\n",
    "    rewind_module = None\n",
    "    for n, m in rewind_model.named_modules():\n",
    "        if name == n:\n",
    "            rewind_module = m\n",
    "            break\n",
    "    pre_module_weight_shape = module.weight.shape\n",
    "    pre_module_bias_shape = module.bias.shape\n",
    "    # Prune in_channel\n",
    "    if name not in ['entry']:\n",
    "        temp_module = copy.deepcopy(module)\n",
    "        prune.ln_structured(temp_module, 'weight', amount=0.90625, n=2, dim=1)\n",
    "        prune.remove(temp_module, 'weight')\n",
    "        in_channel_mask = temp_module.weight.sum(-1).sum(-1).sum(0) != 0\n",
    "    else:\n",
    "        in_channel_mask = torch.ones(module.weight.shape[1], dtype=torch.bool).to(device)\n",
    "    # Prune out_channel\n",
    "    if name not in ['exit']:\n",
    "        temp_module = copy.deepcopy(module)\n",
    "        prune.ln_structured(temp_module, 'weight', amount=0.90625, n=2, dim=0)\n",
    "        prune.remove(temp_module, 'weight')\n",
    "        out_channel_mask = temp_module.weight.sum(-1).sum(-1).sum(-1) != 0\n",
    "    else:\n",
    "        out_channel_mask = torch.ones(module.weight.shape[0], dtype=torch.bool).to(device)\n",
    "    module.weight = torch.nn.Parameter(rewind_module.weight[:, in_channel_mask])\n",
    "    module.weight = torch.nn.Parameter(module.weight[out_channel_mask, :])\n",
    "    module.bias = torch.nn.Parameter(rewind_module.bias[out_channel_mask])\n",
    "    logger.debug(f\"mask_index.shape: {out_channel_mask.shape}\")\n",
    "    logger.debug(f\"module.weight.shape: {pre_module_weight_shape} --> {module.weight.shape}\")\n",
    "    logger.debug(f\"module.bias.shape: {pre_module_bias_shape} --> {module.bias.shape}\")\n",
    "logger.info('Params number(After prune): {}'.format(count_num_of_parameters(model_new)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune MSRResNet\n",
    "\n",
    "import copy\n",
    "\n",
    "model_new = copy.deepcopy(model)\n",
    "logger.info('Params number(Before prune): {}'.format(count_num_of_parameters(model_new)))\n",
    "pre_mask_index = torch.ones(3, dtype=torch.bool).to(device)\n",
    "conv_first_mask_index = None\n",
    "for name, module in model_new.named_modules():\n",
    "    if 'conv' in name:\n",
    "        if name == 'upconv2':\n",
    "            logger.debug(\"=\" * 20)\n",
    "            logger.debug(f\"{name}: Unpruned\")\n",
    "            continue\n",
    "        # DEBUG ----------------------------------------------------\n",
    "        logger.debug(\"=\" * 20)\n",
    "        logger.debug(f\"{name}: Pruned\")\n",
    "        pre_module_weight_shape = module.weight.shape\n",
    "        pre_module_bias_shape = module.bias.shape\n",
    "        # DEBUG ----------------------------------------------------\n",
    "        # Prune in_channel\n",
    "        if name == 'upconv1':\n",
    "            logger.debug(f\"conv_first_mask_index.shape: {conv_first_mask_index.shape}\")\n",
    "            module.weight = torch.nn.Parameter(module.weight[:, conv_first_mask_index])\n",
    "            # DEBUG ----------------------------------------------------\n",
    "            logger.debug(f\"mask_index.shape: {mask_index.shape}\")\n",
    "            logger.debug(f\"module.weight.shape: {pre_module_weight_shape} --> {module.weight.shape}\")\n",
    "            logger.debug(f\"module.bias.shape: {pre_module_bias_shape} --> {module.bias.shape}\")\n",
    "            # DEBUG ----------------------------------------------------\n",
    "            continue\n",
    "        if name not in ['conv_first', 'HRconv']: # Without first\n",
    "            logger.debug(f\"pre_mask_index.shape: {pre_mask_index.shape}\")\n",
    "            module.weight = torch.nn.Parameter(module.weight[:, pre_mask_index])\n",
    "        # Get pruning mask\n",
    "        prune.ln_structured(module, 'weight', amount=0.9, n=2, dim=0)\n",
    "        prune.remove(module, 'weight')\n",
    "        mask_index = module.weight.sum(-1).sum(-1).sum(-1) != 0\n",
    "        pre_mask_index = mask_index\n",
    "        if name == 'conv_first':\n",
    "            conv_first_mask_index = mask_index\n",
    "        # Prune out_channel\n",
    "        if name not in ['conv_last']: # Without last\n",
    "            module.weight = torch.nn.Parameter(module.weight[mask_index, :])\n",
    "            module.bias = torch.nn.Parameter(module.bias[mask_index])\n",
    "            # DEBUG ----------------------------------------------------\n",
    "            logger.debug(f\"mask_index.shape: {mask_index.shape}\")\n",
    "            logger.debug(f\"module.weight.shape: {pre_module_weight_shape} --> {module.weight.shape}\")\n",
    "            logger.debug(f\"module.bias.shape: {pre_module_bias_shape} --> {module.bias.shape}\")\n",
    "            # DEBUG ----------------------------------------------------\n",
    "    else:\n",
    "        logger.debug(\"=\" * 20)\n",
    "        logger.debug(f\"{name}: Unpruned\")\n",
    "logger.info('Params number(After prune): {}'.format(count_num_of_parameters(model_new)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model_new, L_folder, P_folder, logger, True)\n",
    "#test(model, L_folder, P_folder, logger, True)\n",
    "logger.info(f'PSNR after pruning {calculate_psnr(H_folder, P_folder, logger)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image\n",
    "img_lr = torch.ones((1, 3, 32, 32)).to(device)\n",
    "img_hr = torch.ones((1, 3, 32 * 4, 32 * 4)).to(device)\n",
    "criterion = torch.nn.MSELoss()\n",
    "# Train model a little bit\n",
    "optimizer = torch.optim.Adam(pruned_model.parameters())\n",
    "for j in range(50):\n",
    "    optimizer.zero_grad()\n",
    "    img_sr = pruned_model(img_lr)\n",
    "    loss = criterion(img_hr, img_sr)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"[{j + 1}/{50}] loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_model_path = \"model/RRDB_32_srflow2_finetune.pth\"\n",
    "torch.save({\n",
    "    'net': pruned_model.state_dict()\n",
    "}, pruned_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
